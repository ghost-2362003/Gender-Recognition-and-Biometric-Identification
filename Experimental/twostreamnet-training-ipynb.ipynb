{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12352613,"sourceType":"datasetVersion","datasetId":7787677},{"sourceId":12478926,"sourceType":"datasetVersion","datasetId":7830258}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install guided-filter-pytorch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T02:14:35.373057Z","iopub.execute_input":"2025-07-16T02:14:35.373407Z","iopub.status.idle":"2025-07-16T02:14:39.931207Z","shell.execute_reply.started":"2025-07-16T02:14:35.373383Z","shell.execute_reply":"2025-07-16T02:14:39.930319Z"}},"outputs":[{"name":"stdout","text":"Collecting guided-filter-pytorch\n  Downloading guided_filter_pytorch-3.7.5-py3-none-any.whl.metadata (1.6 kB)\nDownloading guided_filter_pytorch-3.7.5-py3-none-any.whl (3.8 kB)\nInstalling collected packages: guided-filter-pytorch\nSuccessfully installed guided-filter-pytorch-3.7.5\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.cuda as cuda\nfrom torchvision import transforms\nfrom torchvision.datasets import ImageFolder\nfrom torch import nn\nfrom torchvision.models.alexnet import AlexNet_Weights\nimport torchvision\nimport numpy as np\nimport cv2\nfrom guided_filter_pytorch.guided_filter import GuidedFilter\nimport time\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T02:14:39.932359Z","iopub.execute_input":"2025-07-16T02:14:39.932933Z","iopub.status.idle":"2025-07-16T02:14:47.939679Z","shell.execute_reply.started":"2025-07-16T02:14:39.932894Z","shell.execute_reply":"2025-07-16T02:14:47.938724Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"**Creating the functions for extracting the high and low freqency image components**","metadata":{}},{"cell_type":"code","source":"def createLowFrequencyComponent(img, guided_filter_Radius = 10):\n\n    image = cv2.imread(img)\n    grayscale_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    img_tensor = torch.from_numpy(image).float().permute(2, 0, 1).unsqueeze(0) / 255.0\n    gray_tensor = torch.from_numpy(grayscale_image).float().unsqueeze(0).unsqueeze(0) / 255.0\n\n    # Use the already defined hr_x (GuidedFilter instance)\n    GF = GuidedFilter(r=guided_filter_Radius, eps=0.01)\n\n    low_freq_image = GF(gray_tensor, img_tensor)\n    low_freq_image = low_freq_image.squeeze(0).permute(1, 2, 0)    ## convert tensor to proper image dimensions\n    low_freq_image = low_freq_image.numpy()     ## convert tensor to numpy array\n\n    return low_freq_image\n\ndef createHighFrequencyComponent(img, epsilon=0.01):\n\n    image = cv2.imread(img)\n    eps = np.full((1200, 1600, 3), epsilon)     ## for numerical stability\n    eps_tensor = torch.from_numpy(eps).float().permute(0, 1, 2)     ## convert eps to tensor\n\n    # create the low frequency image\n    low_freq_image = createLowFrequencyComponent(img)\n    low_freq_image = torch.from_numpy(low_freq_image)\n\n    # create the high frequency image\n    high_frequency_image = image/(low_freq_image + eps_tensor)\n    Ih_yuv = cv2.cvtColor(high_frequency_image.detach().numpy(), cv2.COLOR_RGB2YUV)\n    Y = Ih_yuv[:, :, 0]\n    high_frequency_image = (Y - Y.min()) / (Y.max() - Y.min())\n\n    return high_frequency_image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T02:14:47.941695Z","iopub.execute_input":"2025-07-16T02:14:47.942058Z","iopub.status.idle":"2025-07-16T02:14:47.952115Z","shell.execute_reply.started":"2025-07-16T02:14:47.942035Z","shell.execute_reply":"2025-07-16T02:14:47.951292Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"**Creating the first stream**","metadata":{}},{"cell_type":"code","source":"class FirstStream(nn.Module):\n    def __init__(self):\n        super(FirstStream, self).__init__()\n\n        # Load pretrained AlexNet\n        alexnet = torchvision.models.alexnet(pretrained=AlexNet_Weights.DEFAULT)\n\n        # Use AlexNet features (conv1 to conv5)\n        self.features = alexnet.features  # Conv layers\n\n        # Use AlexNet fc6 and fc7\n        self.fc6 = alexnet.classifier[0]  # Linear(9216, 4096)\n        self.relu6 = alexnet.classifier[1]\n        self.dropout6 = alexnet.classifier[2]\n\n        self.fc7 = alexnet.classifier[3]  # Linear(4096, 4096)\n        self.relu7 = alexnet.classifier[4]\n        self.dropout7 = alexnet.classifier[5]\n\n        # Custom fc8 and fc9 layers\n        self.fc8 = nn.Linear(4096, 2048)\n        self.relu8 = nn.ReLU()\n        self.dropout8 = nn.Dropout(p=0.5)\n\n        self.fc9 = nn.Linear(2048, 531)\n\n    def forward(self, x):\n        x = self.features(x)              # conv1–conv5\n        x = torch.flatten(x, 1)           # Flatten to (B, 9216)\n\n        x = self.fc6(x)\n        x = self.relu6(x)\n        x = self.dropout6(x)\n\n        x = self.fc7(x)\n        x = self.relu7(x)\n        x = self.dropout7(x)\n\n        x = self.fc8(x)\n        x = self.relu8(x)\n        x = self.dropout8(x)\n\n        x = self.fc9(x)\n        \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T02:14:47.956021Z","iopub.execute_input":"2025-07-16T02:14:47.956289Z","iopub.status.idle":"2025-07-16T02:14:47.994417Z","shell.execute_reply.started":"2025-07-16T02:14:47.956246Z","shell.execute_reply":"2025-07-16T02:14:47.993806Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"alex_mod = torchvision.models.alexnet(weights = AlexNet_Weights.DEFAULT)\nconv_1 = alex_mod.features[0]\n\n# get the weights of the 1st conv layer\nweights = conv_1.weight\nnum_filters = weights.shape[0]\nnum_color_channels = weights.shape[1]\n\n# change the shape of the conv_1 layer\n\nbefore_luma_weights = torch.zeros(64, 3, 121)\nfor i in range(num_filters):\n    temp = weights[i].reshape(weights[i].size(0), -1)\n    before_luma_weights[i] = temp\n\n# compute the luma weights\nluma_weights = torch.zeros((num_filters, 121, 1))       ## initalize the luma_weights\nluma_components = torch.tensor([[0.2989, 0.578, 0.114]])  ## luma components for RGB to grayscale conversion\n\nfor i in range(num_filters):\n    temp = before_luma_weights[i].T @ luma_components.T\n    luma_weights[i] = temp\n\nalex_mod.features[0].weight = torch.nn.Parameter(luma_weights.reshape(64, 1, 11, 11))      # set the new luma weights to the conv2d_1 layer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T02:14:47.995080Z","iopub.execute_input":"2025-07-16T02:14:47.995285Z","iopub.status.idle":"2025-07-16T02:14:50.082822Z","shell.execute_reply.started":"2025-07-16T02:14:47.995242Z","shell.execute_reply":"2025-07-16T02:14:50.082166Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n100%|██████████| 233M/233M [00:01<00:00, 196MB/s] \n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"**Creating the second stream**","metadata":{}},{"cell_type":"code","source":"class SecondStream(nn.Module):\n    def __init__(self):\n        super(SecondStream, self).__init__()\n        \n        modified_alexnet = alex_mod\n        \n        self.features = modified_alexnet.features # conv layers\n        \n        # Use AlexNet fc6 and fc7\n        self.fc6 = modified_alexnet.classifier[0]  # Linear(9216, 4096)\n        self.relu6 = modified_alexnet.classifier[1]\n        self.dropout6 = modified_alexnet.classifier[2]\n\n        self.fc7 = modified_alexnet.classifier[3]  # Linear(4096, 4096)\n        self.relu7 = modified_alexnet.classifier[4]\n        self.dropout7 = modified_alexnet.classifier[5]\n        \n        self.fc8 = nn.Linear(4096, 2048)\n        self.relu8 = nn.ReLU()\n        self.dropout8 = nn.Dropout(p=0.5)\n        \n        self.fc9 = nn.Linear(2048, 2048)\n        self.relu9 = nn.ReLU()\n        self.dropout9 = nn.Dropout(p=0.5)\n        \n        self.fc10 = nn.Linear(2048, 531)\n        \n    def forward(self, x):\n        x = self.features(x)\n        \n        x = torch.flatten(x, 1)  # Flatten to (B, 9216)\n        \n        x = self.fc6(x)\n        x = self.relu6(x)\n        x = self.dropout6(x)\n        \n        x = self.fc7(x)\n        x = self.relu7(x)\n        x = self.dropout7(x)\n        \n        x = self.fc8(x)\n        x = self.relu8(x)\n        x = self.dropout8(x)\n        \n        x = self.fc9(x)\n        x = self.relu9(x)\n        x = self.dropout9(x)\n        \n        x = self.fc10(x)\n        \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T02:14:50.083582Z","iopub.execute_input":"2025-07-16T02:14:50.083903Z","iopub.status.idle":"2025-07-16T02:14:50.091702Z","shell.execute_reply.started":"2025-07-16T02:14:50.083876Z","shell.execute_reply":"2025-07-16T02:14:50.090804Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"**Defining both the stream models**","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if cuda.is_available() else \"cpu\")\n\nstream_1 = FirstStream()\nstream_2 = SecondStream()\n\n# load the state dictionaries\nstream_1.load_state_dict(torch.load('/kaggle/input/stream-models/stream1_model.pth'), strict=False)\nstream_2.load_state_dict(torch.load('/kaggle/input/stream-models/stream2_model.pth'), strict=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T02:14:50.092779Z","iopub.execute_input":"2025-07-16T02:14:50.093077Z","iopub.status.idle":"2025-07-16T02:14:54.937381Z","shell.execute_reply.started":"2025-07-16T02:14:50.093051Z","shell.execute_reply":"2025-07-16T02:14:54.936759Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"_IncompatibleKeys(missing_keys=[], unexpected_keys=['modfc.weight', 'modfc.bias'])"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"**Creating the dataloader for the joint Stream**","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 32\ndata_root = '/kaggle/input/11k-hands-training-dataset/content/drive/MyDrive/train_images/train'\n\n# override the ImageFolder to include the custom function\nclass CustomImageFolder(ImageFolder):\n    def __init__(self, root, transform=None):\n        super().__init__(root=root, transform=None)  # disable transform for now\n        self.base_transform = transform  # keep your transform pipeline without the custom fn\n\n    def __getitem__(self, index):\n        path, target = self.samples[index]\n\n        # custom arg is the image itself in your case\n        blurred_img = createLowFrequencyComponent(path)\n        blurred_img = (blurred_img - blurred_img.min())/(blurred_img.max() - blurred_img.min())\n        blurred_img = torch.from_numpy(blurred_img).permute(2, 0, 1).float()\n\n        detailed_img = createHighFrequencyComponent(path)\n        detailed_img = cv2.resize(detailed_img, (224, 224))\n        detailed_img = np.expand_dims(detailed_img, axis=0)  # shape: (1, 224, 224)\n        detailed_img = torch.from_numpy(detailed_img).float()\n        \n        if self.base_transform is not None:\n            blurred_img = self.base_transform(blurred_img)\n            detailed_img = self.base_transform(detailed_img)\n\n        return blurred_img, detailed_img, target\n\nbase_transform = transforms.Compose([\n    transforms.Resize((224, 224))\n])\n\ndataset = CustomImageFolder(root=data_root, transform=base_transform)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T02:14:54.939179Z","iopub.execute_input":"2025-07-16T02:14:54.939420Z","iopub.status.idle":"2025-07-16T02:15:00.361465Z","shell.execute_reply.started":"2025-07-16T02:14:54.939402Z","shell.execute_reply":"2025-07-16T02:15:00.360691Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"**Creating the joint model: TwoStreamNet**","metadata":{}},{"cell_type":"code","source":"class TwoStreamNet(nn.Module):\n    def __init__(self, FirstStream, SecondStream): \n        \n        super(TwoStreamNet, self).__init__()\n        self.stream1 = FirstStream\n        self.stream2 = SecondStream\n        \n        self.sequential = nn.Sequential(\n            nn.Linear(in_features=1062, out_features=1062),\n            nn.Unflatten(1, (1, 1062)),\n            nn.AvgPool1d(kernel_size=2, stride=2),\n            nn.Flatten(), \n            nn.Linear(in_features=531, out_features=2),\n            nn.Softmax(dim=1)\n        )\n\n    def forward(self, blurred_img, detailed_img):\n        f1 = self.stream1(blurred_img)\n        f2 = self.stream2(detailed_img)\n\n        x = torch.concat((f1, f2), dim=1)\n        x = self.sequential(x)\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T02:15:00.362199Z","iopub.execute_input":"2025-07-16T02:15:00.362482Z","iopub.status.idle":"2025-07-16T02:15:00.367812Z","shell.execute_reply.started":"2025-07-16T02:15:00.362459Z","shell.execute_reply":"2025-07-16T02:15:00.367059Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"**Training the joint model**","metadata":{}},{"cell_type":"code","source":"model = TwoStreamNet(stream_1, stream_2).to(device)\nmodel.load_state_dict(torch.load('/kaggle/input/stream-models/joint_model.pth'))\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.002, momentum=0.9)\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0.0\n    correct = 0\n    total = 0\n    loop = tqdm(dataloader, total=len(dataloader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n    \n    for (blurred_img, detailed_img, labels) in loop:\n        blurred_img, detailed_img, labels = blurred_img.to(device), detailed_img.to(device), labels.to(device)\n\n        outputs = model(blurred_img, detailed_img)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item() * blurred_img.size(0)\n        _, predicted = torch.max(outputs, 1)\n        correct += (predicted == labels).sum().item()\n        total += labels.size(0)\n            \n        loop.set_postfix(loss=loss.item(), acc=correct/total)\n        \n    print(f\"For Epoch {epoch+1} — Accuracy: {correct/total:.4f}, Loss: {total_loss/total:.4f}\")\n    torch.save(model.state_dict(), \"/kaggle/working/joint_model.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), \"/kaggle/working/joint_model.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}