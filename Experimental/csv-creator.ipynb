{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12352613,"sourceType":"datasetVersion","datasetId":7787677},{"sourceId":12478926,"sourceType":"datasetVersion","datasetId":7830258},{"sourceId":12789810,"sourceType":"datasetVersion","datasetId":8086249}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/essentials')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T15:24:53.706815Z","iopub.execute_input":"2025-08-18T15:24:53.707071Z","iopub.status.idle":"2025-08-18T15:24:53.716489Z","shell.execute_reply.started":"2025-08-18T15:24:53.707044Z","shell.execute_reply":"2025-08-18T15:24:53.715691Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install guided-filter-pytorch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T15:24:53.718285Z","iopub.execute_input":"2025-08-18T15:24:53.718633Z","iopub.status.idle":"2025-08-18T15:24:58.525615Z","shell.execute_reply.started":"2025-08-18T15:24:53.718604Z","shell.execute_reply":"2025-08-18T15:24:58.524397Z"}},"outputs":[{"name":"stdout","text":"Collecting guided-filter-pytorch\n  Downloading guided_filter_pytorch-3.7.5-py3-none-any.whl.metadata (1.6 kB)\nDownloading guided_filter_pytorch-3.7.5-py3-none-any.whl (3.8 kB)\nInstalling collected packages: guided-filter-pytorch\nSuccessfully installed guided-filter-pytorch-3.7.5\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom stream1 import FirstStream\nfrom stream2 import SecondStream\nfrom TwoStreamNet import TwoStreamNet\nfrom img_converter import createHighFrequencyComponent, createLowFrequencyComponent\nimport torch\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom torchvision.datasets import ImageFolder\nfrom torchvision import transforms\nfrom tqdm import tqdm\nfrom pathlib import Path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T15:24:58.526795Z","iopub.execute_input":"2025-08-18T15:24:58.527171Z","iopub.status.idle":"2025-08-18T15:25:10.361424Z","shell.execute_reply.started":"2025-08-18T15:24:58.527133Z","shell.execute_reply":"2025-08-18T15:25:10.360494Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n 33%|███▎      | 77.1M/233M [00:00<00:00, 169MB/s] ","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"path = Path()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"device being used: {device}\")\n\n# create the stream1 and stream2 model objects and load model file\nstream_1 = FirstStream()\nstream_2 = SecondStream()\nstream_1.load_state_dict(torch.load('/kaggle/input/stream-models/stream1_model.pth',\n                                    map_location=device\n                                    ), \n                         strict=False)\nstream_2.load_state_dict(torch.load('/kaggle/input/stream-models/stream2_model.pth', \n                                    map_location=device\n                                    ), \n                         strict=False)\n\nclassifier = TwoStreamNet(stream_1, stream_2).to(device)\nclassifier.load_state_dict(torch.load('/kaggle/input/stream-models/joint_model.pth', \n                                      map_location=device\n                                      ), \n                           strict=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T15:25:10.362512Z","iopub.execute_input":"2025-08-18T15:25:10.362898Z","iopub.status.idle":"2025-08-18T15:25:21.978546Z","shell.execute_reply.started":"2025-08-18T15:25:10.362878Z","shell.execute_reply":"2025-08-18T15:25:21.977721Z"}},"outputs":[{"name":"stdout","text":"device being used: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# create the dataloader\nBATCH_SIZE = 1\ndata_root = '/kaggle/input/11k-hands-training-dataset/content/drive/MyDrive/train_images/train'\n#test_root = Path(\"G:/11k_hands/dataset/test\")\nvectors = []\nlabels = []\n\n# override the ImageFolder to include the custom function\nclass CustomImageFolder(ImageFolder):\n    def __init__(self, root, transform=None):\n        super().__init__(root=root, transform=None)  # disable transform for now\n        self.base_transform = transform  # keep the transform pipeline without the custom fn\n\n    def __getitem__(self, index):\n        path, target = self.samples[index]\n\n        blurred_img = createLowFrequencyComponent(path)\n        blurred_img = (blurred_img - blurred_img.min())/(blurred_img.max() - blurred_img.min())\n        blurred_img = torch.from_numpy(blurred_img).permute(2, 0, 1).float()\n\n        detailed_img = createHighFrequencyComponent(path)\n        detailed_img = cv2.resize(detailed_img, (224, 224))\n        detailed_img = np.expand_dims(detailed_img, axis=0)  # shape: (1, 224, 224)\n        detailed_img = torch.from_numpy(detailed_img).float()\n        \n        if self.base_transform is not None:\n            blurred_img = self.base_transform(blurred_img)\n            detailed_img = self.base_transform(detailed_img)\n\n        return blurred_img, detailed_img, target\n\nbase_transform = transforms.Compose([\n    transforms.Resize((224, 224))\n])\n\ndataset = CustomImageFolder(root=data_root, transform=base_transform)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T15:25:21.979591Z","iopub.execute_input":"2025-08-18T15:25:21.979903Z","iopub.status.idle":"2025-08-18T15:25:30.493951Z","shell.execute_reply.started":"2025-08-18T15:25:21.979881Z","shell.execute_reply":"2025-08-18T15:25:30.493068Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"#store each of the vectors to a csv file\nclassifier.eval()\nloop = tqdm(dataloader, total=len(dataloader))\n\nfor (blurred_img, detailed_img, label) in loop:\n    blurred_img, detailed_img, label = blurred_img.to(device), detailed_img.to(device), label.to(device)\n\n    f1, f2, x = classifier(blurred_img, detailed_img)\n    concatenated_feature = torch.concat([f1, f2], dim=1)\n    concatenated_feature = torch.concat([concatenated_feature, x], dim=1)\n    \n    vectors.append(concatenated_feature.cpu().detach().numpy())\n    labels.append(label.cpu().detach().numpy())\n\n# save the features and labels to a file\nvectors = np.array(vectors)\nvectors = vectors.squeeze(axis=1)\ndf = pd.DataFrame(vectors)\ndf['Label'] = labels\n\ndf.to_csv('/kaggle/working/features.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T15:25:30.496252Z","iopub.execute_input":"2025-08-18T15:25:30.496586Z","iopub.status.idle":"2025-08-18T17:22:48.310778Z","shell.execute_reply.started":"2025-08-18T15:25:30.496563Z","shell.execute_reply":"2025-08-18T17:22:48.308854Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 7754/7754 [1:57:17<00:00,  1.10it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/747539319.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# save the features and labels to a file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    865\u001b[0m                     )\n\u001b[1;32m    866\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m                     mgr = ndarray_to_mgr(\n\u001b[0m\u001b[1;32m    868\u001b[0m                         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m                         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;31m# by definition an array here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;31m# the dtypes will be coerced to a single dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prep_ndarraylike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy_on_sanitize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_prep_ndarraylike\u001b[0;34m(values, copy)\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ensure_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_ensure_2d\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Must pass 2-d input. shape={values.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Must pass 2-d input. shape=(7754, 1, 2124)"],"ename":"ValueError","evalue":"Must pass 2-d input. shape=(7754, 1, 2124)","output_type":"error"}],"execution_count":6}]}