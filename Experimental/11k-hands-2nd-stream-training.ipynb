{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12352613,"sourceType":"datasetVersion","datasetId":7787677},{"sourceId":12385129,"sourceType":"datasetVersion","datasetId":7804468}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install guided-filter-pytorch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T13:55:12.107433Z","iopub.execute_input":"2025-07-10T13:55:12.107654Z","iopub.status.idle":"2025-07-10T13:55:16.456269Z","shell.execute_reply.started":"2025-07-10T13:55:12.107629Z","shell.execute_reply":"2025-07-10T13:55:16.455262Z"}},"outputs":[{"name":"stdout","text":"Collecting guided-filter-pytorch\n  Downloading guided_filter_pytorch-3.7.5-py3-none-any.whl.metadata (1.6 kB)\nDownloading guided_filter_pytorch-3.7.5-py3-none-any.whl (3.8 kB)\nInstalling collected packages: guided-filter-pytorch\nSuccessfully installed guided-filter-pytorch-3.7.5\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport cv2\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.cuda as cuda\nimport torchvision\nfrom torchvision import transforms\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.models.alexnet import AlexNet_Weights\nfrom guided_filter_pytorch.guided_filter import GuidedFilter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T13:55:16.457328Z","iopub.execute_input":"2025-07-10T13:55:16.458265Z","iopub.status.idle":"2025-07-10T13:55:23.728296Z","shell.execute_reply.started":"2025-07-10T13:55:16.458227Z","shell.execute_reply":"2025-07-10T13:55:23.727732Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"**Functions to create the low and high frequency components**","metadata":{}},{"cell_type":"code","source":"def createLowFrequencyComponent(img, guided_filter_Radius = 10):\n\n    image = cv2.imread(img)\n    grayscale_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    img_tensor = torch.from_numpy(image).float().permute(2, 0, 1).unsqueeze(0) / 255.0\n    gray_tensor = torch.from_numpy(grayscale_image).float().unsqueeze(0).unsqueeze(0) / 255.0\n\n    # Use the already defined hr_x (GuidedFilter instance)\n    GF = GuidedFilter(r=guided_filter_Radius, eps=0.01)\n\n    low_freq_image = GF(gray_tensor, img_tensor)\n    low_freq_image = low_freq_image.squeeze(0).permute(1, 2, 0)    ## convert tensor to proper image dimensions\n    low_freq_image = low_freq_image.numpy()     ## convert tensor to numpy array\n\n    return low_freq_image\n\ndef createHighFrequencyComponent(img, epsilon=0.01):\n\n    image = cv2.imread(img)\n    eps = np.full((1200, 1600, 3), epsilon)     ## for numerical stability\n    eps_tensor = torch.from_numpy(eps).float().permute(0, 1, 2)     ## convert eps to tensor\n\n    # create the low frequency image\n    low_freq_image = createLowFrequencyComponent(img)\n    low_freq_image = torch.from_numpy(low_freq_image)\n\n    # create the high frequency image\n    high_frequency_image = image/(low_freq_image + eps_tensor)\n    Ih_yuv = cv2.cvtColor(high_frequency_image.detach().numpy(), cv2.COLOR_RGB2YUV)\n    Y = Ih_yuv[:, :, 0]\n    high_frequency_image = (Y - Y.min()) / (Y.max() - Y.min())\n\n    return high_frequency_image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T13:55:23.729995Z","iopub.execute_input":"2025-07-10T13:55:23.730575Z","iopub.status.idle":"2025-07-10T13:55:23.737196Z","shell.execute_reply.started":"2025-07-10T13:55:23.730554Z","shell.execute_reply":"2025-07-10T13:55:23.736479Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"print(torch.__version__)\nprint(torchvision.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T13:55:23.740288Z","iopub.execute_input":"2025-07-10T13:55:23.740518Z","iopub.status.idle":"2025-07-10T13:55:23.758520Z","shell.execute_reply.started":"2025-07-10T13:55:23.740502Z","shell.execute_reply":"2025-07-10T13:55:23.757854Z"}},"outputs":[{"name":"stdout","text":"2.6.0+cu124\n0.21.0+cu124\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"**Changing the weights of the first convolutional layer of alexnet architecture**","metadata":{}},{"cell_type":"code","source":"alex_mod = torchvision.models.alexnet(weights = AlexNet_Weights.DEFAULT)\nconv_1 = alex_mod.features[0]\n\n# get the weights of the 1st conv layer\nweights = conv_1.weight\nnum_filters = weights.shape[0]\nnum_color_channels = weights.shape[1]\n\n# change the shape of the conv_1 layer\n\nbefore_luma_weights = torch.zeros(64, 3, 121)\nfor i in range(num_filters):\n    temp = weights[i].reshape(weights[i].size(0), -1)\n    before_luma_weights[i] = temp\n\n# compute the luma weights\nluma_weights = torch.zeros((num_filters, 121, 1))       ## initalize the luma_weights\nluma_components = torch.tensor([[0.2989, 0.578, 0.114]])  ## luma components for RGB to grayscale conversion\n\nfor i in range(num_filters):\n    temp = before_luma_weights[i].T @ luma_components.T\n    luma_weights[i] = temp\n\nalex_mod.features[0].weight = torch.nn.Parameter(luma_weights.reshape(64, 1, 11, 11))      # set the new luma weights to the conv2d_1 layer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T13:55:23.759215Z","iopub.execute_input":"2025-07-10T13:55:23.759467Z","iopub.status.idle":"2025-07-10T13:55:25.817340Z","shell.execute_reply.started":"2025-07-10T13:55:23.759442Z","shell.execute_reply":"2025-07-10T13:55:25.816501Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n100%|██████████| 233M/233M [00:01<00:00, 219MB/s] \n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from torch import nn\n\n# implement the second stream\nclass ModifiedSecondStream(nn.Module):\n    def __init__(self):\n        super(ModifiedSecondStream, self).__init__()\n        \n        modified_alexnet = alex_mod\n        \n        self.features = modified_alexnet.features # conv layers\n        \n        # Use AlexNet fc6 and fc7\n        self.fc6 = modified_alexnet.classifier[0]  # Linear(9216, 4096)\n        self.relu6 = modified_alexnet.classifier[1]\n        self.dropout6 = modified_alexnet.classifier[2]\n\n        self.fc7 = modified_alexnet.classifier[3]  # Linear(4096, 4096)\n        self.relu7 = modified_alexnet.classifier[4]\n        self.dropout7 = modified_alexnet.classifier[5]\n        \n        self.fc8 = nn.Linear(4096, 2048)\n        self.relu8 = nn.ReLU()\n        self.dropout8 = nn.Dropout(p=0.5)\n        \n        self.fc9 = nn.Linear(2048, 2048)\n        self.relu9 = nn.ReLU()\n        self.dropout9 = nn.Dropout(p=0.5)\n        \n        self.fc10 = nn.Linear(2048, 531)\n        self.modfc = nn.Linear(531, 2)\n        self.softmax = nn.Softmax(dim=1)\n        \n    def forward(self, x):\n        x = self.features(x)\n        \n        x = torch.flatten(x, 1)  # Flatten to (B, 9216)\n        \n        x = self.fc6(x)\n        x = self.relu6(x)\n        x = self.dropout6(x)\n        \n        x = self.fc7(x)\n        x = self.relu7(x)\n        x = self.dropout7(x)\n        \n        x = self.fc8(x)\n        x = self.relu8(x)\n        x = self.dropout8(x)\n        \n        x = self.fc9(x)\n        x = self.relu9(x)\n        x = self.dropout9(x)\n        \n        x = self.fc10(x)\n        x = self.modfc(x)\n        x = self.softmax(x)\n        \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T13:55:25.818178Z","iopub.execute_input":"2025-07-10T13:55:25.818789Z","iopub.status.idle":"2025-07-10T13:55:25.826607Z","shell.execute_reply.started":"2025-07-10T13:55:25.818765Z","shell.execute_reply":"2025-07-10T13:55:25.825957Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"IMG_SIZE = 224\nBATCH_SIZE = 24\ndata_root = '/kaggle/input/11k-hands-training-dataset/content/drive/MyDrive/train_images/train'\n\n# override the ImageFolder to include the custom fucntion\nclass CustomImageFolder(ImageFolder):\n    def __init__(self, root, transform=None):\n        super().__init__(root=root, transform=None)  # disable transform for now\n        self.base_transform = transform  # keep your transform pipeline without the custom fn\n\n    def __getitem__(self, index):\n        path, target = self.samples[index]\n\n        # custom arg is the image itself in your case\n        img = createHighFrequencyComponent(path)\n        img = cv2.resize(img, (224, 224))\n        img = np.expand_dims(img, axis=0)  # shape: (1, 224, 224)\n        img = torch.from_numpy(img).float()  # convert to  tensor\n        \n        if self.base_transform is not None:\n            img = self.base_transform(img)\n\n        return img, target\n\ndataset = CustomImageFolder(root=data_root)\ndataloader_stream2 = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\nclasses = dataset.classes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T13:55:25.827394Z","iopub.execute_input":"2025-07-10T13:55:25.827591Z","iopub.status.idle":"2025-07-10T13:55:31.944385Z","shell.execute_reply.started":"2025-07-10T13:55:25.827576Z","shell.execute_reply":"2025-07-10T13:55:31.943486Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"device = torch.device(\"cuda\" if cuda.is_available() else \"cpu\")\n\nmodel = ModifiedSecondStream()\nmodel.load_state_dict(torch.load('/kaggle/input/11k-hands-stream2-model/stream2_model.pth'))\n\nmodified_second_stream = model.to(device)\n\npretrained_params = (\n    list(modified_second_stream .features.parameters())\n    + list(modified_second_stream.fc6.parameters()) + list(modified_second_stream.relu6.parameters()) + list(modified_second_stream.dropout6.parameters())\n    + list(modified_second_stream.fc7.parameters()) + list(modified_second_stream.relu7.parameters()) + list(modified_second_stream.dropout7.parameters())\n    + list(modified_second_stream.fc8.parameters()) + list(modified_second_stream.relu8.parameters()) + list(modified_second_stream.dropout8.parameters())\n    + list(modified_second_stream.fc9.parameters()) + list(modified_second_stream.relu9.parameters()) + list(modified_second_stream.dropout9.parameters())\n    + list(modified_second_stream.fc10.parameters())\n)\n\nnew_fc_params = list(modified_second_stream.modfc.parameters()) + list(modified_second_stream.softmax.parameters())\n\noptimizer = optim.SGD([\n    {'params': pretrained_params, 'lr': 1e-4},\n    {'params': new_fc_params, 'lr': 0.002}\n], momentum=0.9)\n\ncriterion = nn.CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T13:55:31.946787Z","iopub.execute_input":"2025-07-10T13:55:31.947024Z","iopub.status.idle":"2025-07-10T13:55:35.653244Z","shell.execute_reply.started":"2025-07-10T13:55:31.947007Z","shell.execute_reply":"2025-07-10T13:55:35.652482Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"**Creating the Training Loop**","metadata":{}},{"cell_type":"code","source":"import time\nfrom tqdm import tqdm\n\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    modified_second_stream.train()\n    total_loss = 0.0\n    correct = 0\n    total = 0\n\n    # Start timing this epoch\n    start_time = time.time()\n\n    # tqdm progress bar for this epoch\n    loop = tqdm(dataloader_stream2, total=len(dataloader_stream2), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n\n    for images, labels in loop:\n        images, labels = images.to(device), labels.to(device)\n\n        outputs = modified_second_stream(images)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item() * images.size(0)\n        _, predicted = torch.max(outputs, 1)\n        correct += (predicted == labels).sum().item()\n        total += labels.size(0)\n\n        # Update progress bar with latest metrics\n        loop.set_postfix(loss=loss.item(), acc=correct / total)\n\n    # End timing\n    epoch_time = time.time() - start_time\n    print(f\"Epoch {epoch+1} completed in {epoch_time:.2f} sec — Accuracy: {correct/total:.4f}, Loss: {total_loss/total:.4f}\")\n    torch.save(modified_second_stream.state_dict(), \"/kaggle/working/stream2_model.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T13:55:35.654063Z","iopub.execute_input":"2025-07-10T13:55:35.654308Z","execution_failed":"2025-07-10T13:56:06.906Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/10:   1%|          | 2/324 [00:27<1:13:43, 13.74s/it, acc=0.75, loss=0.584] ","output_type":"stream"}],"execution_count":null}]}