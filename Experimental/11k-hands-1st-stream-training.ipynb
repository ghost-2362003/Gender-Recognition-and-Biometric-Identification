{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12352613,"sourceType":"datasetVersion","datasetId":7787677},{"sourceId":12358350,"sourceType":"datasetVersion","datasetId":7787845}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n       # print(os.path.join(dirname, filename))\n        pass\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-03T02:03:46.612333Z","iopub.execute_input":"2025-07-03T02:03:46.612619Z","iopub.status.idle":"2025-07-03T02:03:54.165888Z","shell.execute_reply.started":"2025-07-03T02:03:46.612597Z","shell.execute_reply":"2025-07-03T02:03:54.165351Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!pip install guided-filter-pytorch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T02:03:54.166920Z","iopub.execute_input":"2025-07-03T02:03:54.167285Z","iopub.status.idle":"2025-07-03T02:03:58.269424Z","shell.execute_reply.started":"2025-07-03T02:03:54.167267Z","shell.execute_reply":"2025-07-03T02:03:58.268713Z"}},"outputs":[{"name":"stdout","text":"Collecting guided-filter-pytorch\n  Downloading guided_filter_pytorch-3.7.5-py3-none-any.whl.metadata (1.6 kB)\nDownloading guided_filter_pytorch-3.7.5-py3-none-any.whl (3.8 kB)\nInstalling collected packages: guided-filter-pytorch\nSuccessfully installed guided-filter-pytorch-3.7.5\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch.optim as optim\nimport torch.nn.functional as F\nimport torch.cuda as cuda\nfrom torchvision import transforms\nfrom torchvision.datasets import ImageFolder\nimport torchvision","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T02:03:58.270424Z","iopub.execute_input":"2025-07-03T02:03:58.270677Z","iopub.status.idle":"2025-07-03T02:04:05.357614Z","shell.execute_reply.started":"2025-07-03T02:03:58.270652Z","shell.execute_reply":"2025-07-03T02:04:05.357100Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport cv2\nfrom guided_filter_pytorch.guided_filter import GuidedFilter\n\ndef createLowFrequencyComponent(img, guided_filter_Radius = 10):\n\n    image = cv2.imread(img)\n    grayscale_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    img_tensor = torch.from_numpy(image).float().permute(2, 0, 1).unsqueeze(0) / 255.0\n    gray_tensor = torch.from_numpy(grayscale_image).float().unsqueeze(0).unsqueeze(0) / 255.0\n\n    # Use the already defined hr_x (GuidedFilter instance)\n    GF = GuidedFilter(r=guided_filter_Radius, eps=0.01)\n\n    low_freq_image = GF(gray_tensor, img_tensor)\n    low_freq_image = low_freq_image.squeeze(0).permute(1, 2, 0)    ## convert tensor to proper image dimensions\n    low_freq_image = low_freq_image.numpy()     ## convert tensor to numpy array\n\n    return low_freq_image\n\ndef createHighFrequencyComponent(img, epsilon=0.01):\n\n    image = cv2.imread(img)\n    eps = np.full((1200, 1600, 3), epsilon)     ## for numerical stability\n    eps_tensor = torch.from_numpy(eps).float().permute(0, 1, 2)     ## convert eps to tensor\n\n    # create the low frequency image\n    low_freq_image = createLowFrequencyComponent(img)\n    low_freq_image = torch.from_numpy(low_freq_image)\n\n    # create the high frequency image\n    high_frequency_image = image/(low_freq_image + eps_tensor)\n    Ih_yuv = cv2.cvtColor(high_frequency_image.detach().numpy(), cv2.COLOR_RGB2YUV)\n    Y = Ih_yuv[:, :, 0]\n    high_frequency_image = (Y - Y.min()) / (Y.max() - Y.min())\n\n    return high_frequency_image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T02:04:05.359461Z","iopub.execute_input":"2025-07-03T02:04:05.359751Z","iopub.status.idle":"2025-07-03T02:04:05.611368Z","shell.execute_reply.started":"2025-07-03T02:04:05.359735Z","shell.execute_reply":"2025-07-03T02:04:05.610827Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"print(torch.__version__)\nprint(torchvision.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T02:04:05.612036Z","iopub.execute_input":"2025-07-03T02:04:05.612277Z","iopub.status.idle":"2025-07-03T02:04:05.615937Z","shell.execute_reply.started":"2025-07-03T02:04:05.612253Z","shell.execute_reply":"2025-07-03T02:04:05.615287Z"}},"outputs":[{"name":"stdout","text":"2.6.0+cu124\n0.21.0+cu124\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import os\nimport zipfile\n\n# Path where Kaggle stores user-uploaded datasets\ndataset_path = \"/kaggle/input/11k-hands-training-dataset\"\n\n'''# Unzip (if needed)\nwith zipfile.ZipFile(f\"{dataset_path}\", 'r') as zip_ref:\n    zip_ref.extractall(\"/kaggle/working/\")'''\n\n# Final path\ndata_root = \"/kaggle/input/11k-hands-training-dataset/content/drive/MyDrive/train_images/train\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T02:04:05.616581Z","iopub.execute_input":"2025-07-03T02:04:05.616806Z","iopub.status.idle":"2025-07-03T02:04:05.630623Z","shell.execute_reply.started":"2025-07-03T02:04:05.616787Z","shell.execute_reply":"2025-07-03T02:04:05.629992Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"IMG_SIZE = 224\nBATCH_SIZE = 32\n\n# override the ImageFolder to include the custom fucntion\nclass CustomImageFolder(ImageFolder):\n    def __init__(self, root, transform=None):\n        super().__init__(root=root, transform=None)  # disable transform for now\n        self.base_transform = transform  # keep your transform pipeline without the custom fn\n\n    def __getitem__(self, index):\n        path, target = self.samples[index]\n\n        # custom arg is the image itself in your case\n        img = createLowFrequencyComponent(path)\n        img = (img - img.min())/(img.max() - img.min())\n        img = torch.from_numpy(img).permute(2, 0, 1).float()\n\n        if self.base_transform is not None:\n            img = self.base_transform(img)\n\n        return img, target\n\nbase_transform = transforms.Compose([\n    transforms.Resize((224, 224))\n])\n\ndataset = CustomImageFolder(root=data_root, transform=base_transform)\ndataloader_stream1 = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\nclasses = dataset.classes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T02:04:05.631237Z","iopub.execute_input":"2025-07-03T02:04:05.631426Z","iopub.status.idle":"2025-07-03T02:04:08.271007Z","shell.execute_reply.started":"2025-07-03T02:04:05.631411Z","shell.execute_reply":"2025-07-03T02:04:08.270443Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from torch import nn\nfrom torchvision.models.alexnet import AlexNet_Weights\n\nclass ModifiedFirstStream(nn.Module):\n    def __init__(self):\n        super(ModifiedFirstStream, self).__init__()\n\n        # Load pretrained AlexNet\n        alexnet = torchvision.models.alexnet(pretrained=AlexNet_Weights.DEFAULT)\n\n        # Use AlexNet features (conv1 to conv5)\n        self.features = alexnet.features  # Conv layers\n\n        # Use AlexNet fc6 and fc7\n        self.fc6 = alexnet.classifier[0]  # Linear(9216, 4096)\n        self.relu6 = alexnet.classifier[1]\n        self.dropout6 = alexnet.classifier[2]\n\n        self.fc7 = alexnet.classifier[3]  # Linear(4096, 4096)\n        self.relu7 = alexnet.classifier[4]\n        self.dropout7 = alexnet.classifier[5]\n\n        # Custom fc8 and fc9 layers\n        self.fc8 = nn.Linear(4096, 2048)\n        self.relu8 = nn.ReLU()\n        self.dropout8 = nn.Dropout(p=0.5)\n\n        self.fc9 = nn.Linear(2048, 531)\n\n        self.modfc = nn.Linear(531, 2)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = self.features(x)              # conv1â€“conv5\n        x = torch.flatten(x, 1)           # Flatten to (B, 9216)\n\n        x = self.fc6(x)\n        x = self.relu6(x)\n        x = self.dropout6(x)\n\n        x = self.fc7(x)\n        x = self.relu7(x)\n        x = self.dropout7(x)\n\n        x = self.fc8(x)\n        x = self.relu8(x)\n        x = self.dropout8(x)\n\n        x = self.fc9(x)\n        x = self.modfc(x)\n        x = self.softmax(x)\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T02:04:08.271717Z","iopub.execute_input":"2025-07-03T02:04:08.271963Z","iopub.status.idle":"2025-07-03T02:04:08.279155Z","shell.execute_reply.started":"2025-07-03T02:04:08.271924Z","shell.execute_reply":"2025-07-03T02:04:08.278451Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"device = torch.device(\"cuda\" if cuda.is_available() else \"cpu\")\n\nmodel = ModifiedFirstStream()\nmodel.load_state_dict(torch.load('/kaggle/input/stream1-model-state-dict/stream1_model.pth'))\n\nmodified_first_stream = model.to(device)\n\npretrained_params = (\n    list(modified_first_stream.features.parameters())\n    + list(modified_first_stream.fc6.parameters()) + list(modified_first_stream.relu6.parameters()) + list(modified_first_stream.dropout6.parameters())\n    + list(modified_first_stream.fc7.parameters()) + list(modified_first_stream.relu7.parameters()) + list(modified_first_stream.dropout7.parameters())\n    + list(modified_first_stream.fc8.parameters()) + list(modified_first_stream.relu8.parameters()) + list(modified_first_stream.dropout8.parameters())\n    + list(modified_first_stream.fc9.parameters())\n)\n\nnew_fc_params = list(modified_first_stream.modfc.parameters()) + list(modified_first_stream.softmax.parameters())\n\noptimizer = optim.SGD([\n    {'params': pretrained_params, 'lr': 1e-4},\n    {'params': new_fc_params, 'lr': 0.002}\n], momentum=0.9)\n\ncriterion = nn.CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T02:04:08.279747Z","iopub.execute_input":"2025-07-03T02:04:08.279940Z","iopub.status.idle":"2025-07-03T02:04:13.421121Z","shell.execute_reply.started":"2025-07-03T02:04:08.279927Z","shell.execute_reply":"2025-07-03T02:04:13.420434Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 233M/233M [00:01<00:00, 204MB/s] \n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import time\nfrom tqdm import tqdm\n\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    modified_first_stream.train()\n    total_loss = 0.0\n    correct = 0\n    total = 0\n\n    # Start timing this epoch\n    start_time = time.time()\n\n    # tqdm progress bar for this epoch\n    loop = tqdm(dataloader_stream1, total=len(dataloader_stream1), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n\n    for images, labels in loop:\n        images, labels = images.to(device), labels.to(device)\n\n        outputs = modified_first_stream(images)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item() * images.size(0)\n        _, predicted = torch.max(outputs, 1)\n        correct += (predicted == labels).sum().item()\n        total += labels.size(0)\n\n        # Update progress bar with latest metrics\n        loop.set_postfix(loss=loss.item(), acc=correct / total)\n\n    # End timing\n    epoch_time = time.time() - start_time\n    print(f\"Epoch {epoch+1} completed in {epoch_time:.2f} sec â€” Accuracy: {correct/total:.4f}, Loss: {total_loss/total:.4f}\")\n    torch.save(modified_first_stream.state_dict(), \"/kaggle/working/stream1_model.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T02:04:13.422800Z","iopub.execute_input":"2025-07-03T02:04:13.423010Z","iopub.status.idle":"2025-07-03T06:11:30.519342Z","shell.execute_reply.started":"2025-07-03T02:04:13.422994Z","shell.execute_reply":"2025-07-03T06:11:30.518459Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 243/243 [51:30<00:00, 12.72s/it, acc=0.918, loss=0.432] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 completed in 3090.65 sec â€” Accuracy: 0.9176, Loss: 0.3984\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 243/243 [49:10<00:00, 12.14s/it, acc=0.924, loss=0.531]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 completed in 2950.88 sec â€” Accuracy: 0.9235, Loss: 0.3917\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 243/243 [47:15<00:00, 11.67s/it, acc=0.928, loss=0.345]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 completed in 2835.22 sec â€” Accuracy: 0.9282, Loss: 0.3864\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 243/243 [48:31<00:00, 11.98s/it, acc=0.935, loss=0.411]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 completed in 2911.63 sec â€” Accuracy: 0.9347, Loss: 0.3788\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 243/243 [50:45<00:00, 12.53s/it, acc=0.937, loss=0.317]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 completed in 3045.36 sec â€” Accuracy: 0.9369, Loss: 0.3759\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"torch.save(modified_first_stream.state_dict(), \"/kaggle/working/stream1_model.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T06:11:30.520352Z","iopub.execute_input":"2025-07-03T06:11:30.520686Z","iopub.status.idle":"2025-07-03T06:11:31.128969Z","shell.execute_reply.started":"2025-07-03T06:11:30.520658Z","shell.execute_reply":"2025-07-03T06:11:31.128253Z"}},"outputs":[],"execution_count":12}]}